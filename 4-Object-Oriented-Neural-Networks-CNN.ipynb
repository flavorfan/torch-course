{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ML Pipeline__: Prepare data -> __build model__ -> train model -> analyze model's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOP review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we’re writing programs or building software, there are two key components, __code__ and __data__. With object oriented programming, we orient our program design and structure around objects.\n",
    "\n",
    "Objects are defined in code using classes. A class defines the object's specification or spec, which specifies what data and code each object of the class should have. When we create an object of a class, we call the object an instance of the class, and all instances of a given class have two core components:\n",
    "\n",
    "* Methods (code)\n",
    "* Attributes (data)\n",
    "\n",
    "The methods represent the code, while the attributes represent the data, and so the methods and attributes are defined by the class.\n",
    "\n",
    "In a given program, many objects, a.k.a instances of a given class, can exist simultaneously, and all of the instances will have the same available attributes and the same available methods. They are uniform from this perspective. The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be encapsulated within the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components:\n",
    "\n",
    "* A transformation (code)\n",
    "* A collection of weights (data)\n",
    "\n",
    "Within the `nn` package, there is a class called `Module`, and it is the __base class__ for all of neural network modules which includes layers. This means that all of the layers in PyTorch extend the `nn.Module` class and inherit all of PyTorch’s built-in functionality within the `nn.Module` class. In OOP this concept is known as __inheritance__.\n",
    "\n",
    "\n",
    "#### `forward()` method\n",
    "When we pass a tensor to our network as input, the __tensor flows__ forward though each layer transformation until the tensor reaches the output layer. This process of a tensor flowing forward though the network is known as a __forward pass__.\n",
    "\n",
    "Each layer has its own transformation (code) and the tensor passes forward through each layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network. The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction. This is achieved efficiently by __backpropagation__.\n",
    "\n",
    "What this all means is that, every PyTorch `nn.Module` has a `forward()` method, and so when we are building layers and networks, we must provide an implementation of the `forward()` method. The forward method is the actual transformation.\n",
    "\n",
    "\n",
    "#### PyTorch’s `nn.functional` package\n",
    "\n",
    "The `nn.functional` package contains methods that subclasses of `nn.Module` use for implementing their `forward()` functions. One reason for this is that during backpropagation, the network must perform a __symbolic differentiation__ of the operations involved in the layers to calculate the gradient of the loss with respect to the weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network in PyTorch\n",
    "\n",
    "To build a neural net in PyTorch one must extend the nn.Module base class. This is done in two steps:\n",
    "2. In the class constructor, define the network’s layers as class attributes using pre-built layers from `torch.nn`.\n",
    "3. Use the network’s layer attributes as well as operations from the `nn.functional` API to implement the `forward()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# a trivial neural net (zero layers)\n",
    "class Network(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = None\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convolutional neural net\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6,  kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        # dense layers / fully connected layers with bias\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # the 4*4 is the size of the image \n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)     # after two convolutions and maxpoolings\n",
    "        self.out = nn.Linear(in_features=60,  out_features=10)  \n",
    "        \n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above neural net has three hyperparameters that need to be manually specified:\n",
    "* `kernel_size.`  size of each convolutional filter\n",
    "* `out_channels` number of filters in the convolutional layer \n",
    "* `out_features` size of output tensor, i.e. the number of neurons in the dense layer\n",
    "\n",
    "Having `out_features=10` on the final output layer is a data dependent hyperparameter, i.e. fixed due to the nature of the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring weights inside layers\n",
    "\n",
    "Note that weights are hidden inside each layer object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Network()\n",
    "network # __repr__ is inherited from nn.Module, compiles all __repr__ of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=192, out_features=120, bias=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=60, out_features=10, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-1.7346e-01,  6.8424e-02,  1.1999e-01, -1.1691e-01,  1.3970e-01],\n",
       "          [ 9.0718e-02, -1.2584e-01, -6.7626e-02, -4.4363e-02, -1.8667e-01],\n",
       "          [ 1.7493e-01, -1.5471e-01,  1.5186e-01,  4.9944e-02,  5.2649e-02],\n",
       "          [ 2.4085e-02, -1.8250e-02,  1.1244e-01,  1.1311e-01, -1.2301e-01],\n",
       "          [-1.7065e-01,  1.9091e-01, -3.1706e-02,  4.6051e-02,  1.0087e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5561e-02, -1.3326e-01,  8.8125e-02, -1.0556e-01,  1.2639e-02],\n",
       "          [ 7.0812e-02,  1.2028e-01,  1.5602e-01,  1.4910e-01,  8.5042e-02],\n",
       "          [ 1.9760e-01,  1.4803e-01, -2.6618e-02,  3.7042e-03, -1.3785e-01],\n",
       "          [-5.2191e-02, -1.8784e-02,  1.9702e-01,  1.6927e-01,  5.1905e-02],\n",
       "          [ 1.3098e-01, -3.2314e-03,  1.9275e-01, -1.5166e-01,  6.7232e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.6703e-02,  6.9931e-02, -1.3542e-01,  6.0347e-02,  1.6653e-01],\n",
       "          [ 1.9688e-01,  1.2755e-01,  1.0265e-02,  4.0055e-02,  5.0993e-02],\n",
       "          [-1.8140e-01, -1.6846e-01, -1.6384e-01,  1.7868e-01,  1.8877e-01],\n",
       "          [ 1.9359e-01,  5.4731e-02, -1.0323e-01,  5.5850e-02,  1.7621e-01],\n",
       "          [-9.8523e-03, -1.8943e-01, -7.4054e-02,  1.0574e-01,  2.7173e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.4046e-01,  3.0944e-03,  1.1768e-01, -9.4425e-02, -1.4424e-01],\n",
       "          [-5.7718e-02, -7.1236e-03, -1.8277e-01,  1.5443e-01, -6.8831e-02],\n",
       "          [ 1.2908e-01,  1.9570e-01, -1.5571e-01,  1.1390e-01,  1.2615e-01],\n",
       "          [ 7.3041e-02,  1.0354e-01, -9.0845e-02, -2.4888e-02, -5.0534e-02],\n",
       "          [ 1.6572e-01, -1.9655e-01, -1.3980e-01,  6.8487e-02,  1.7152e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.6667e-01,  1.2618e-01,  5.3780e-02, -1.4107e-01,  1.4310e-01],\n",
       "          [ 3.8608e-02, -5.3644e-05,  9.7063e-03,  4.6594e-02,  1.5717e-01],\n",
       "          [ 1.5070e-01,  3.0597e-02, -4.8430e-02, -1.7392e-01,  1.4391e-01],\n",
       "          [-8.8040e-02,  4.2071e-02, -6.4075e-02,  1.7483e-01, -6.2421e-02],\n",
       "          [ 1.4033e-01,  5.4208e-02, -1.9527e-01, -5.1975e-02, -1.4698e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.3612e-02, -6.1769e-02,  7.2132e-02, -8.6431e-02,  8.3529e-02],\n",
       "          [ 7.4478e-02,  1.8445e-01, -6.7203e-02, -1.9824e-02,  1.0076e-01],\n",
       "          [-1.9818e-01, -1.0500e-01,  1.8228e-01,  4.9224e-02, -1.6675e-01],\n",
       "          [ 1.1341e-01,  5.3892e-02, -1.2552e-01, -5.7536e-03, -6.5717e-02],\n",
       "          [ 5.1226e-02, -2.6136e-02,  1.5296e-01, -8.8613e-02, -1.6842e-01]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight # weights of the convolutional layer; these are actually the six  5x5 convolutional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filters as images (untrained, randomized)\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for i in range(1,7):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.imshow(network.conv1.weight[0].squeeze().detach().numpy(), cmap='gray')\n",
    "    plt.title(f'Filter {i-1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the twelve 5$\\times$5 stacked filters convolve above six stacked input images which are the output of the first convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = np.random.random(192)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight.matmul(torch.as_tensor(x).float()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation `A.matmul(x)` infers due to incompatibility in shape that it must perform $x A^T$ instead of the default $Ax$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [0, 0]])\n",
    "\n",
    "x = np.array([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 7, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(A).matmul(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(x, A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([12])\n",
      "fc1.weight \t torch.Size([120, 192])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([60, 120])\n",
      "fc2.bias \t torch.Size([60])\n",
      "out.weight \t torch.Size([10, 60])\n",
      "out.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, weight in network.named_parameters():\n",
    "    print(name, '\\t', weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0580, -0.0491, -0.0361,  ...,  0.0383,  0.0131,  0.0303],\n",
       "        [ 0.0469, -0.0052,  0.0509,  ..., -0.0442,  0.0112, -0.0482],\n",
       "        [-0.0439, -0.0024,  0.0484,  ...,  0.0446,  0.0126, -0.0268],\n",
       "        ...,\n",
       "        [-0.0716, -0.0493,  0.0173,  ...,  0.0496, -0.0392,  0.0085],\n",
       "        [ 0.0210, -0.0194, -0.0699,  ..., -0.0522, -0.0364, -0.0500],\n",
       "        [-0.0165,  0.0591, -0.0677,  ...,  0.0188,  0.0009,  0.0627]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight # 192 weights for each of the 120 output neurons +1 for bias, the bias tensor of size 120."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the convolutional layer, we have one scalar value for each filter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward method implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass describes how an input tensor is processed as it flows across the layers.\n",
    "\n",
    "__tensor flow__: \n",
    "tensor -> input layer -> hiden layers -> output layer -> tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, channels=1): # default grayscale\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # ((28-5+1)/2 -5 +1)/2 = 4\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.forward()` method expects an image of shape $[b, c, 28, 28]$ i.e. $b$ is batch size of 28$\\times$28 grayscale images of shape $[c,28,28]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(3)\n",
    "t = torch.as_tensor(np.random.random((7,3,28,28))).float() # batch of size 7\n",
    "\n",
    "net.forward(t).shape # 10 label classes for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(t) == net.forward(t)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0832, -0.1029,  0.0703,  0.0292,  0.0262],\n",
       "         [-0.0237, -0.0543, -0.0521, -0.0272, -0.0309],\n",
       "         [ 0.0332,  0.0453, -0.0441, -0.1148, -0.0725],\n",
       "         [-0.0449, -0.0101,  0.0459, -0.0727, -0.0317],\n",
       "         [-0.0826, -0.1063,  0.0104, -0.0092, -0.0855]],\n",
       "\n",
       "        [[-0.0777, -0.0996,  0.1076,  0.0575, -0.0444],\n",
       "         [ 0.0145,  0.0996, -0.0795, -0.0624, -0.0244],\n",
       "         [-0.0365,  0.1009,  0.1087,  0.0875,  0.1027],\n",
       "         [-0.0857, -0.0225, -0.0068,  0.0377, -0.0547],\n",
       "         [-0.0984,  0.0819,  0.0161, -0.0403, -0.1129]],\n",
       "\n",
       "        [[-0.0042, -0.0070,  0.0478,  0.0235,  0.1039],\n",
       "         [ 0.0199,  0.0314,  0.0427,  0.0709,  0.0157],\n",
       "         [-0.0739,  0.0840,  0.0589,  0.0240,  0.1063],\n",
       "         [ 0.0590, -0.0079, -0.0406,  0.0153,  0.0723],\n",
       "         [-0.0204,  0.0012, -0.0969, -0.0526,  0.0382]]], requires_grad=True)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 6 filters in the first conv layer are 3x5x5 matrices in this case, threefold\n",
    "print(net.conv1.weight.shape)\n",
    "net.conv1.weight[0] # weights of the first filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor shape transformations during forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we inspect the transformation of a tensor $t$ as it is passed end-to-end into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(3)\n",
    "t = torch.as_tensor(np.random.random((7,3,28,28))).float() # batch of size 7 rgb 28x28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 28, 28])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape # input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 6, 24, 24])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.conv1(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 6, 24, 24])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = F.relu(t) # applied element-wise\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 0]],\n",
       "\n",
       "        [[1, 5]],\n",
       "\n",
       "        [[0, 6]]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(torch.tensor([[[2,-3]], [[1,5]], [[-1,6]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 6, 12, 12])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate how `F.max_pool2d` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[[1,2],[3,4]],[[1,2],[-9,0]]]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.],\n",
       "          [ 3.,  4.]],\n",
       "\n",
       "         [[ 1.,  2.],\n",
       "          [-9.,  0.]]]])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2, 2])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.]],\n",
       "\n",
       "         [[2.]]]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(x, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 1])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(x, kernel_size=2, stride=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 12, 8, 8])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.conv2(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 12, 4, 4])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 192])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.reshape(-1, 12*4*4)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 120])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.fc1(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 60])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.fc2(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = net.out(t)\n",
    "t = F.softmax(t, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 10])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layer output formula\n",
    "$$ O = \\dfrac{f-n+2p}{s}+1$$\n",
    "\n",
    "where $f$ is the image size, $n$ is the size of the filter, $p$ is the size of zero padding. To see this, note that the formula is equal to $\\dfrac{f-n+s+2p}{s}$ which can be interpreted as ignoring the last stride which covers $n$ pixels, and replacing that with $s$ pixels, dividing everything by $s$ we get the total number of strides, i.e. the size of the resulting image.\n",
    "$$$$\n",
    "\n",
    "__Example__. $f = 28$, $n = 5$, $p = 0$ and $s = 1$ in the first convolutional layer, so that $O = 24$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Basic Theory of Convolutional Neural Networks\n",
    "\n",
    "Deep Lizard series on CNN:\n",
    "\n",
    "https://deeplizard.com/learn/video/YRhxdVk_sIs\n",
    "\n",
    "https://deeplizard.com/learn/video/cNBBNAxC8l4\n",
    "\n",
    "https://deeplizard.com/learn/video/qSTv_m-KFk0\n",
    "\n",
    "https://deeplizard.com/learn/video/ZjM_XQa5s6s\n",
    "\n",
    "https://deeplizard.com/learn/video/gmBfb6LNnZs\n",
    "\n",
    "\n",
    "\n",
    "F. Chollet's blog: __How convolutional neural networks see the world__\n",
    "\n",
    "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Callable Neural Networks - Linear Layer in Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "    ],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3421, -0.7038,  0.2216], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3,bias=False)\n",
    "fc.weight = nn.Parameter(weight_matrix)\n",
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. CNN Forward Method - PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, channels=1): # default grayscale\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # ((28-5+1)/2 -5 +1)/2 = 4\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23 CNN Image Prediction with PyTorch - Forward Propagation Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x70b453f36b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = sample\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0930, 0.0950, 0.1061, 0.1005, 0.0960, 0.1033, 0.1017, 0.1122, 0.0936, 0.0986]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = network(image.unsqueeze(0))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0916, 0.0935, 0.0929, 0.1050, 0.1064, 0.1089, 0.1035, 0.0998, 0.0904, 0.1080]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Network()\n",
    "net1(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1009, 0.0984, 0.1045, 0.1097, 0.0892, 0.0983, 0.1007, 0.0990, 0.0972, 0.1022]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = Network()\n",
    "net2(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24 NN Batch Processing - Pass Image Batch to PyTorch CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, channels=1): # default grayscale\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # ((28-5+1)/2 -5 +1)/2 = 4\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x70b37b953ce0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9, 1, 0, 6, 4, 3, 1, 4, 8, 4, 3, 0, 2, 4, 4, 5, 3, 6, 6, 0, 8, 5,\n",
       "         2, 1, 6, 6, 7, 9, 5, 9, 2, 7, 3, 0, 3, 3, 3, 7, 2, 2, 6, 6, 8, 3, 3, 5, 0, 5, 5, 0, 2, 0, 0, 4, 1, 3, 1, 6, 3,\n",
       "         1, 4, 4, 6, 1, 9, 1, 3, 5, 7, 9, 7, 1, 7, 9, 9, 9, 3, 2, 9, 3, 6, 4, 1, 1, 8])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 28, 28]), torch.Size([100]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = batch\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]),\n",
       " tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).shape, preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False,  True, False,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False,  True,  True, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum()   \n",
    "\n",
    "get_num_correct(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 NN Output Formula - NN Debugging Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0198,  0.0333, -0.0858,  0.0843, -0.0089,  0.0437,  0.0102,  0.1014,  0.0048,  0.0661]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, channels=1): # default grayscale\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # ((28-5+1)/2 -5 +1)/2 = 4\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "image, label = sample\n",
    "\n",
    "network = Network()\n",
    "\n",
    "output = network(image.unsqueeze(0))\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
