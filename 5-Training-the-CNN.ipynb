{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pipeline\n",
    "\n",
    "So far in this series, we learned about Tensors, and we've learned all about PyTorch neural networks. We are now ready to begin the training process.\n",
    "\n",
    "1. Prepare the data\n",
    "2. Build the model\n",
    "3. __Train the model__ \n",
    "4. Analyze the model's results\n",
    "\n",
    "Training is essentially gradient descent: we calculate the gradient of the loss, and use this to update the weights so that the resulting loss is closer the the minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "During training, we get a __batch__ and pass it forward through the network. Once the output is obtained, we compare the predicted output to the actual labels, and once we know how close the predicted values are from the actual labels, we tweak the weights inside the network in such a way that the values the network predicts move closer to the true values (labels). All of this is for a single batch, and we repeat this process for every batch until we have covered every sample in our training set. \n",
    "\n",
    "After we've completed this process for all of the batches and passed over every sample in our training set, we say that an __epoch__ is complete. During the entire training process, we do as many epochs as necessary to reach our desired level of accuracy. \n",
    "\n",
    "__Training a neural net:__\n",
    "1. Get batch from the training set.\n",
    "2. Pass batch to network.\n",
    "3. Calculate the __loss__ (difference between the predicted values and the true values).\n",
    "4. Calculate the gradient of the loss function w.r.t the network's weights.\n",
    "5. Update the weights using the gradients to reduce the loss.\n",
    "6. Repeat steps 1-5 until one epoch is completed.\n",
    "7. Repeat steps 1-6 for as many epochs required to reach the minimum loss.\n",
    "\n",
    "We know how to do steps 1 and 2. The loss is specified depending on the problem. We use backpropagation and an optimization algorithm to perform step 4 and 5. Steps 6 and 7 are just standard Python loops (the training loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, channels=1): # default grayscale\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # ((28-5+1)/2 -5 +1)/2 = 4\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST',\n",
    "    download=True,\n",
    "    transform=transform.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "batch = next(iter(train_loader)) # get one batch\n",
    "images, labels = batch\n",
    "\n",
    "# initialize a network\n",
    "network = Network() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return (preds.argmax(dim=1) == labels).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`F.cross_entropy` combines __negative log-loss with softmax and averages the result__. Thus we comment out above the softmax activation. Here we have a batch size of 2. Thus to compare absolute lossess for different batch sizes one must multiply by the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8087)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(torch.tensor([[3, 6, 2], [1, 1, 2]]).float(), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8086643088447403"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "softmax = lambda a: np.exp(a) / np.exp(a).sum()\n",
    "\n",
    "(-np.log(softmax(np.array([3,6,2])))[2] + -np.log(softmax(np.array([1,1,2])))[1])/2 # averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01) # optimizer has access to network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(2.3068, grad_fn=<NllLossBackward>)\n",
      "no. correct: tensor(12)\n"
     ]
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels) \n",
    "print('loss: ', loss) \n",
    "print('no. correct:', get_num_correct(preds, labels)) # out of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward() # backprop, looks at definition of loss and crawls backward into the network\n",
    "network.conv1.weight.grad.shape # gradients updated after one pass of backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step() # based on our new loss gradient values, we update weights accdg to Adam to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after backprop:  tensor(2.2887, grad_fn=<NllLossBackward>) no. correct: tensor(14)\n"
     ]
    }
   ],
   "source": [
    "preds = network(images) # run new predictions\n",
    "loss = F.cross_entropy(preds, labels) \n",
    "print('loss after backprop: ', loss, 'no. correct:', get_num_correct(preds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct predictions have increased after one step of gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect everything in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "2.3104944229125977\n",
      "tensor(11)\n",
      "\n",
      "Step 1:\n",
      "2.2894699573516846\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "# compile the neural net\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# loss\n",
    "loss = F.cross_entropy(network(images), labels)\n",
    "print(\"Step 0:\")\n",
    "print(loss.item())\n",
    "print(get_num_correct(network(images), labels))\n",
    "\n",
    "# backprop\n",
    "loss.backward()  # update gradients\n",
    "optimizer.step() # update weights using gradients to minimize loss\n",
    "\n",
    "# recalculating loss based on new weights\n",
    "loss = F.cross_entropy(network(images), labels)\n",
    "print(\"\\nStep 1:\")\n",
    "print(loss.item())\n",
    "print(get_num_correct(network(images), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: tensor(46885) loss: 346.6361614763737\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100) \n",
    "\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "for batch in train_loader:\n",
    "    images, labels = batch \n",
    "\n",
    "    preds = network(images) \n",
    "    loss = F.cross_entropy(preds, labels) \n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()  # calculate gradients\n",
    "    optimizer.step() # update weights using gradients using adam\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "print(\n",
    "    \"epoch:\", 0, \n",
    "    \"total_correct:\", total_correct, \n",
    "    \"loss:\", total_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the gradients to zero in line 14 is necessary because `loss.backward()` _adds_ the calculated gradients instead of assigning them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: tensor(45570) loss: 380.7395526468754\n",
      "epoch 1 total_correct: tensor(50782) loss: 251.97550985217094\n",
      "epoch 2 total_correct: tensor(51535) loss: 229.5160759538412\n",
      "epoch 3 total_correct: tensor(51801) loss: 220.17312617599964\n",
      "epoch 4 total_correct: tensor(52229) loss: 211.77114780247211\n",
      "epoch 5 total_correct: tensor(52350) loss: 208.3045560270548\n",
      "epoch 6 total_correct: tensor(52407) loss: 206.49326403439045\n",
      "epoch 7 total_correct: tensor(52563) loss: 201.06376388669014\n",
      "epoch 8 total_correct: tensor(52860) loss: 194.65639048814774\n",
      "epoch 9 total_correct: tensor(52807) loss: 196.51216650009155\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch in train_loader:    \n",
    "        images, labels = batch \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels) # check that loss tensor has a gradient attribute\n",
    "                                              # so that line 17 makes sense\n",
    "        optimizer.zero_grad() # set all gradients to zero\n",
    "        loss.backward() # calculate gradient\n",
    "        optimizer.step() # update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8801166666666667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52807/60000 # train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
